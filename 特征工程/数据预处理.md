# 数据预处理
通过特征提取，我们能得到未经处理的特征，这时的特征可能有以下问题：

- 不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这       一问题。

- 信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。

- 定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用one-hot的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。

- 存在缺失值：缺失值需要补充。

- 信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果

**1. 无量纲化**

- 归一化

通常所说的[0,1]或[-1,1]数据缩放即为归一化过程，以[0,1]归一化为例，需要计算特征的最大值和最小值，计算公式为：

$$ x' = \frac{x-Min}{Max-Min} $$

sklearn.preprocessing中包含的MinMaxScaler类即为上式设计。
    
- 标准化

标准化是将数据分布标准化为均值为0，方差为1的高斯分布上，这一步骤在机器学习的特征预处理以及神经网络前向训练时每层的Batch Normalization中均应用广泛，需要提前计算特征数据的均值$ \overline{x} $以及方差$ S $，公式如下：

$$ x' = \frac{x-\overline{x}}{S} $$

sklearn.preprocessing中包含的StandardScaler类即为上式设计。

- 拓展

*Q1：为何需要对特征进行归一化（标准化）处理？*

**- 消除量纲影响，提高精度**

> 通常我们会用距离的概念去衡量与标准值的差距，从而判断好坏。例如，对于一个标准的人体状态，用体重和身高去衡量，单位分别为kg和m，假设标准状态设为A（50,1.75)，则对于另外两组状态B(60,1.80)和C(50,1.90),采用欧式距离下的L2范数计算距离，则有L[A,B] > L[A,C]，但从直观理解B的健康状态是优于C的，因此需要对不同的特征去赋值不同的权重系数，类似的可以应用马氏距离概念。如果对这组状态的分布进行归一化，则能减少这种误判的趋势。

**- 提高梯度下降等算法求解速度**

> 关于这个解释可以参考斯坦福机器学习教程：https://class.coursera.org/ml-003/lecture/21
> 如下图所示，蓝色圆圈表示基于两个特征参数`$\theta_1,\theta_2$`的损失函数`$J(\theta)$`等高线，红线表示迭代过程中特征参数的不同取值。左图中两个特征的区间跨度很大，因此等高线细长尖锐。当使用梯度下降法寻找最优解时，很有可能走“之字型”路线（根据梯度下降定义，垂直等高线逼近），从而导致需要迭代很多次才能收敛；而右图中对原始特征进行归一化，对应的等高线会趋近圆，因此能够在较少的迭代次数中收敛。

![normalization demo](http://images.cnitblog.com/blog2015/522490/201504/192105553858119.png)
Fig. 1 Gradient Descend With and Without Normalization

*Q2：什么是L2范数归一化？*

> L2范数归一化对应于sklearn.preprocessing中的Normalizer类（默认L2），即将特征向量中的每个元素均除以向量的L2范数，公式如下：


$$ x_i' = \frac{x_i}{norm(x)} $$

$$ norm(x) = \sqrt{x_1^2+x_2^2+\cdots+x_n^2} $$

> 显而易见，L2归一化后，其特征向量L2范数为1.

*Q3：归一化和标准化的应用场景？*

> - 概率模型不需要归一化，因为这种模型不关心变量的取值，而是关心变量的分布和变量之间的条件概率。另外，对于以决策树为基底的模型也不适用归一化，以C4.5算法为例，决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比，而信息增益比跟特征是否经过归一化是无关的，因为归一化（标准化）不会改变样本在特征x上的信息增益。
> - 线性回归，支持向量机之类的最优化问题需要归一化，是否归一化主要在于是否关心变量取值。
> - 神经网络需要标准化处理，目的是弱化某些变量对模型产生较大的参数波动影响，且将每层的数据标准化为同样的分布更加有利于网络的快速训练和拟合。
> - 在K近邻算法中，如果不对解释变量进行标准化，那么具有小数量级的解释变量的影响就会微乎其微，无法为新的判断做出贡献。

**2. 定值特征二值化**

定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，具体公式为：

$$x'=
\begin{cases}
1,&\text{$x$>$threshold$}\\
0,&\text{$x$ \leq $threshold&}
\end{cases}$$

sklearn.preprocessing中包含的Binarizer(threshold=int)类即为上式设计。

 $$ 
f(n)=\begin{cases}
n/2, & \text{如果$ x<=2 $}\\
3n+1, & \text{如果$ x>2 $}
\end{cases}
$$

**3. one-hot编码**

在讨论如何进行one-hot(独热编码)之前，首先了解类别型特征以及几种常见的编码方式。

类别型特征主要是指性别（男、女）、血型（A、B、AB、O）等只在有限选项内取值的特征。类别型特征原始输入通常是字符串形式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。

常见的编码方式如下：
- 序号编码
> 序号编码通常用于处理类别间具有大小关系的数据。例如成绩，可以分为低、中、高三档，并且存在“高>中>低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值ID，例如高表示3、中表示2、低表示1，转换后保留了大小关系。
- 独热编码
> 独热编码通常用于处理类别间不具有大小关系的特征。例如血型，一共有4个取值，独热编码会把血型表示为一个4维的稀疏向量，A型血表示为(1,0,0,0)，B型血表示为(0,1,0,0)，AB型血表示为(0,0,1,0)，O型血表示为(0,0,0,1)。
- 二进制编码
> 二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后将类别ID对应的二进制编码作为结果。利用二进制对ID的哈希映射，最终得到0/1特征向量，且维数少于独热编码，节省了存储空间。

- 编码示例

| 血型 | 类别ID | 二进制表示 | 独热编码 |
| :--: | :----: | :--------: | :------: |
|  A   |    1   | 0-0-1|1-0-0-0|
|B|2|0-1-0|0-1-0-0|
|AB|3|0-1-1|0-0-1-0|
|O|4|1-0-0|0-0-0-1|

sklearn.preprocessing中包含的OneHotEncoder类即为上式设计。

*Q：对于类别取值较多的情况使用独热编码需要注意的问题。*
> - 使用稀疏向量节省空间。
> - 配合特征选择降低维度。 

**4. 缺失值计算**

sklearn.preprocessing中包含的Imputer类即为上式设计。

重要参数说明：
> - missing_values：缺失值，可以为整数或NaN，默认为NaN
> - strategy：替换策略，字符串，默认为均值"mean"
>     - "mean"：特征列均值替代
>     - "median"：特征列中位数替代
>     - "most_frequent"：特征列众数替代
> - axis：指定轴数，默认axis=0代表列

> 注意：Imputer类只接受DataFrame类型，使用pandas进行数据类型转化；DataFrame中必须全部为数值属性！

**5. 数据变换**

scikit-learn提供了一个数据变换库，可以实现清洗、缩减、扩展或产生特征表示。

从另一个角度考虑，数据变换就是对原始特征的组合变换。为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征，包括多项式变换、指数函数变换、对数函数变换等。以多项式变换为例，假设原始特征为$x_1,x_2,x_3,x_4$,采用二次多项式进行变换（又称为“度”=2），则新的特征如下：

$$ (x_1',x_2',x_3',x_4',x_5',x_6',x_7',x_8',x_9',x_{10}',x_{11}',x_{12}',x_{13}',x_{14}',x_{15}')= $$

$$ (1,x_1,x_2,x_3,x_4,x_1^2,x_1 \cdot x_2,x_1 \cdot x_3,x_1 \cdot x_4,x_2^2,x_2 \cdot x_3,x_2 \cdot x_4,x_3^2,x_3 \cdot x_4,x_4^2) $$


sklearn.preprocessing中包含的PolynomialFeatures类即为上式设计。

**6. 代码演示**


```python
from sklearn.datasets import load_iris

# 导入IRIS数据集
iris = load_iris()

# 输出特征矩阵
iris.data

# 输出目标向量
iris.target

******

# 均值标准化
from sklearn.preprocessing import StandardScaler
StandardScaler().fit_transform(iris.data)

******

# [0,1]归一化
from sklearn.preprocessing import MinMaxScaler
MinMaxScaler().fit_transform(iris.data)

******

# 定量特征二值化
from sklearn.preprocessing import Binarizer
Binarizer(threshold=3).fit_transform(iris.data)

******

# one-hot编码
from sklearn.preprocessing import OneHotEncoder
OneHotEncoder().fit_transform(iris.target.reshape((-1,1)

******

# 缺失值计算
from numpy import vstack, array, nan
from sklearn.preprocessing import Imputer
Imputer().fit_transform(vstack((array([nan,nan,nan,nan]), iris.data)))

******

# 数据变换
from sklearn.preprocessing import PolynomialFeatures
PolymialFeatures().fit_transform(iris.data)

```
